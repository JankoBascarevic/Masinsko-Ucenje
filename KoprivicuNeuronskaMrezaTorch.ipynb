{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT79yY7DIkUHRcm5dcUL1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JankoBascarevic/Masinsko-Ucenje/blob/main/KoprivicuNeuronskaMrezaTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nhTMEiPX1Gu",
        "outputId": "16d1d0ae-5764-4b97-9d87-81b5eb307c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Accuracy: 0.2910181886367898\n",
            "1 Accuracy: 0.4324645290330646\n",
            "2 Accuracy: 0.528470529408088\n",
            "3 Accuracy: 0.5792862053878367\n",
            "4 Accuracy: 0.607037939871242\n",
            "5 Accuracy: 0.6234764672792049\n",
            "6 Accuracy: 0.6346021626351647\n",
            "7 Accuracy: 0.6436027251703231\n",
            "8 Accuracy: 0.6514782173885868\n",
            "9 Accuracy: 0.657166072879555\n",
            "10 Accuracy: 0.6641040065004062\n",
            "11 Accuracy: 0.6706044127757985\n",
            "12 Accuracy: 0.677229826864179\n",
            "13 Accuracy: 0.6831051940746297\n",
            "14 Accuracy: 0.6865429089318082\n",
            "15 Accuracy: 0.6877304831551972\n",
            "16 Accuracy: 0.6904181511344459\n",
            "17 Accuracy: 0.694855928495531\n",
            "18 Accuracy: 0.699668729295581\n",
            "19 Accuracy: 0.7070441902618914\n",
            "20 Accuracy: 0.7174198387399212\n",
            "21 Accuracy: 0.72973310831927\n",
            "22 Accuracy: 0.739796237264829\n",
            "23 Accuracy: 0.7504219013688356\n",
            "24 Accuracy: 0.7581098818676167\n",
            "25 Accuracy: 0.7652978311144446\n",
            "26 Accuracy: 0.769048065504094\n",
            "27 Accuracy: 0.7733608350521908\n",
            "28 Accuracy: 0.7760485030314395\n",
            "29 Accuracy: 0.7780486280392525\n",
            "30 Accuracy: 0.7798612413275829\n",
            "31 Accuracy: 0.7818613663353959\n",
            "32 Accuracy: 0.7829239327457966\n",
            "33 Accuracy: 0.7847365460341271\n",
            "34 Accuracy: 0.7877992374523408\n",
            "35 Accuracy: 0.7901118819926245\n",
            "36 Accuracy: 0.7935495968498031\n",
            "37 Accuracy: 0.7969873117069817\n",
            "38 Accuracy: 0.8001125070316895\n",
            "39 Accuracy: 0.802300143758985\n",
            "40 Accuracy: 0.8046127882992687\n",
            "41 Accuracy: 0.8072379523720232\n",
            "42 Accuracy: 0.8080505031564473\n",
            "43 Accuracy: 0.8096131008188012\n",
            "44 Accuracy: 0.8121757609850616\n",
            "45 Accuracy: 0.8139258703668979\n",
            "46 Accuracy: 0.8151759484967811\n",
            "47 Accuracy: 0.8169260578786174\n",
            "48 Accuracy: 0.8179261203825239\n",
            "49 Accuracy: 0.8199887492968311\n",
            "50 Accuracy: 0.8214263391461967\n",
            "51 Accuracy: 0.8226764172760798\n",
            "52 Accuracy: 0.8240515032189512\n",
            "53 Accuracy: 0.825551596974811\n",
            "54 Accuracy: 0.826801675104694\n",
            "55 Accuracy: 0.8279267454215888\n",
            "56 Accuracy: 0.8284267766735421\n",
            "57 Accuracy: 0.8298643665229077\n",
            "58 Accuracy: 0.8309894368398025\n",
            "59 Accuracy: 0.8321770110631914\n",
            "60 Accuracy: 0.8329895618476155\n",
            "61 Accuracy: 0.8338021126320395\n",
            "62 Accuracy: 0.8343646477904869\n",
            "63 Accuracy: 0.8353647102943934\n",
            "64 Accuracy: 0.8358647415463466\n",
            "65 Accuracy: 0.8363647727982999\n",
            "66 Accuracy: 0.8372398274892181\n",
            "67 Accuracy: 0.8377398587411713\n",
            "68 Accuracy: 0.8384899056191012\n",
            "69 Accuracy: 0.8389274329645603\n",
            "70 Accuracy: 0.8393024564035252\n",
            "71 Accuracy: 0.8399899993749609\n",
            "72 Accuracy: 0.8404900306269142\n",
            "73 Accuracy: 0.840802550159385\n",
            "74 Accuracy: 0.8412400775048441\n",
            "75 Accuracy: 0.8414900931308207\n",
            "76 Accuracy: 0.8419901243827739\n",
            "77 Accuracy: 0.8425526595412213\n",
            "78 Accuracy: 0.8431776986061629\n",
            "79 Accuracy: 0.8437402337646103\n",
            "80 Accuracy: 0.8442402650165636\n",
            "81 Accuracy: 0.8448653040815051\n",
            "82 Accuracy: 0.8453028314269642\n",
            "83 Accuracy: 0.8457403587724233\n",
            "84 Accuracy: 0.8461778861178824\n",
            "85 Accuracy: 0.8467404212763298\n",
            "86 Accuracy: 0.8469279329958123\n",
            "87 Accuracy: 0.8471154447152947\n",
            "88 Accuracy: 0.8477404837802363\n",
            "89 Accuracy: 0.8479279954997188\n",
            "90 Accuracy: 0.8481155072192011\n",
            "91 Accuracy: 0.8483030189386837\n",
            "92 Accuracy: 0.848865554097131\n",
            "93 Accuracy: 0.848865554097131\n",
            "94 Accuracy: 0.8491780736296018\n",
            "95 Accuracy: 0.8493655853490844\n",
            "96 Accuracy: 0.8497406087880492\n",
            "97 Accuracy: 0.8501156322270141\n",
            "98 Accuracy: 0.8504281517594849\n",
            "99 Accuracy: 0.8507406712919557\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch as t\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn import *\n",
        "from sklearn.model_selection import train_test_split as split\n",
        "\n",
        "train, test = split( pd.read_csv(\"sample_data/mnist_train_small.csv\").rename(columns={\"6\": \"label\"}), test_size=0.2)\n",
        "train = np.array(train).T\n",
        "#test = np.array(test).T\n",
        "\n",
        "X = t.from_numpy((train[1:] / 255).T).float()\n",
        "Y = t.from_numpy(train[0]).long()\n",
        "\n",
        "class Model(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = Sequential( Linear(784, 150), ReLU(), Linear(150, 10), Softmax() )\n",
        "    def pred(self, x):\n",
        "        return t.argmax( self.net(x), dim=1)\n",
        "\n",
        "model=Model()\n",
        "\n",
        "def fit(loss_fn, opt):\n",
        "    ohy = F.one_hot(Y.to(t.int64), 10).float()\n",
        "    for i in range(100):\n",
        "        loss_fn(model.net(X), ohy).backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        print(i, \"Accuracy:\", t.sum(model.pred(X) == Y).item() / len(Y))\n",
        "\n",
        "fit(F.cross_entropy, t.optim.Adam(model.parameters(), lr=1e-3) )"
      ]
    }
  ]
}